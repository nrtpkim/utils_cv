{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('../assets/cctv-feed-ml_v4_batch4-12site-add-post-prd-bkf-022024.pt')\n",
    "list_step = [0.5, 0.65, 0.8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.29 ðŸš€ Python-3.8.18 torch-2.2.0+cpu CPU (AMD Ryzen 7 4800HS with Radeon Graphics)\n",
      "Model summary (fused): 268 layers, 43608150 parameters, 0 gradients, 164.8 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\works\\Axons\\code\\utils_cv\\assets\\crop-img\\NKA\\labels... 33 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 270.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\works\\Axons\\code\\utils_cv\\assets\\crop-img\\NKA\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:23<00:00,  7.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         46          1      0.283      0.642      0.563\n",
      "                person         33         33          1      0.182      0.591      0.433\n",
      "                 phone         33         13          1      0.385      0.692      0.692\n",
      "Speed: 3.8ms preprocess, 647.1ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val24\u001b[0m\n",
      "Ultralytics YOLOv8.1.29 ðŸš€ Python-3.8.18 torch-2.2.0+cpu CPU (AMD Ryzen 7 4800HS with Radeon Graphics)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\works\\Axons\\code\\utils_cv\\assets\\crop-img\\NKA\\labels.cache... 33 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:21<00:00,  7.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         46          1      0.268      0.634      0.588\n",
      "                person         33         33          1      0.152      0.576      0.484\n",
      "                 phone         33         13          1      0.385      0.692      0.692\n",
      "Speed: 2.9ms preprocess, 593.2ms inference, 0.0ms loss, 0.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val25\u001b[0m\n",
      "Ultralytics YOLOv8.1.29 ðŸš€ Python-3.8.18 torch-2.2.0+cpu CPU (AMD Ryzen 7 4800HS with Radeon Graphics)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\works\\Axons\\code\\utils_cv\\assets\\crop-img\\NKA\\labels.cache... 33 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:22<00:00,  7.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         46          1      0.238      0.619      0.619\n",
      "                person         33         33          1     0.0909      0.545      0.545\n",
      "                 phone         33         13          1      0.385      0.692      0.692\n",
      "Speed: 2.1ms preprocess, 625.1ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val26\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for x in list_step:\n",
    "    validation_results = model.val(data='../assets/val.yaml',conf=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Yolo\n",
    "# model = YOLO('yolov8l.pt')\n",
    "# list_step = [0.5, 0.65, 0.8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in list_step:\n",
    "#     validation_results = model.val(data='../assets/val.yaml',conf=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting\n",
    "FOCUS_SITE = 'NKA'\n",
    "OUT_PATH = f'{FOCUS_SITE}_test'\n",
    "MAIN_DATA_FOLDER = f'../assets/crop-img/{FOCUS_SITE}/images'\n",
    "\n",
    "\n",
    "os.makedirs(os.path.join('output_pred',OUT_PATH), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../assets/crop-img/NKA/images\\\\NKA-ch8_1.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_2.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_3.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_4.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_6.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_7.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_8.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_9.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_22.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_25.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_26.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_27.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_28.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_29.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_30.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_34.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_35.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_36.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_39.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_40.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_41.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_42.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_43.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_44.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_45.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_46.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_47.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_48.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_49.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_50.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_59.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_64.jpg', '../assets/crop-img/NKA/images\\\\NKA-ch8_76.jpg']\n",
      "['NKA-ch8_1.jpg', 'NKA-ch8_2.jpg', 'NKA-ch8_3.jpg', 'NKA-ch8_4.jpg', 'NKA-ch8_6.jpg', 'NKA-ch8_7.jpg', 'NKA-ch8_8.jpg', 'NKA-ch8_9.jpg', 'NKA-ch8_22.jpg', 'NKA-ch8_25.jpg', 'NKA-ch8_26.jpg', 'NKA-ch8_27.jpg', 'NKA-ch8_28.jpg', 'NKA-ch8_29.jpg', 'NKA-ch8_30.jpg', 'NKA-ch8_34.jpg', 'NKA-ch8_35.jpg', 'NKA-ch8_36.jpg', 'NKA-ch8_39.jpg', 'NKA-ch8_40.jpg', 'NKA-ch8_41.jpg', 'NKA-ch8_42.jpg', 'NKA-ch8_43.jpg', 'NKA-ch8_44.jpg', 'NKA-ch8_45.jpg', 'NKA-ch8_46.jpg', 'NKA-ch8_47.jpg', 'NKA-ch8_48.jpg', 'NKA-ch8_49.jpg', 'NKA-ch8_50.jpg', 'NKA-ch8_59.jpg', 'NKA-ch8_64.jpg', 'NKA-ch8_76.jpg']\n"
     ]
    }
   ],
   "source": [
    "### Get data img path\n",
    "image_paths = []\n",
    "files_name = []\n",
    "for root, dirs, files in os.walk(MAIN_DATA_FOLDER):\n",
    "    for file in files:\n",
    "        \n",
    "        if file.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\", \".webp\", \".avif\")):\n",
    "            image_path = os.path.join(root, file)\n",
    "            files_name.append(file)\n",
    "            image_paths.append(image_path)\n",
    "\n",
    "print(image_paths)\n",
    "print(files_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load model\n",
    "model = YOLO('../assets/cctv-feed-ml_v4_batch4-12site-add-post-prd-bkf-022024.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run batch\n",
    "# model.predict(image_paths, conf=0.5, project=\"output_pred\",name=\"BBF_test\",save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [00:26,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "detection_score = 0.5\n",
    "class_dict = {0:'person', 1:'phone'}\n",
    "class_color_dict = {0:(255, 0, 0), \n",
    "                                 1:(0, 0, 255),\n",
    "                                 'area_normal':(0,128,128),\n",
    "                                 'area_alert':(0,165,255)\n",
    "                                 }\n",
    "print(len(files_name))\n",
    "for img_path, file_name in tqdm(zip(image_paths, files_name)):\n",
    "\n",
    "    # read _ img\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    image_height, image_width, channels = img.shape\n",
    "\n",
    "    # Inference\n",
    "    result = model(img, verbose=False)\n",
    "    \n",
    "    cls = result[0].boxes.cls.cpu().numpy()    # cls, (N, 1)\n",
    "    probs = result[0].boxes.conf.cpu().numpy()  # confidence score, (N, 1)\n",
    "    boxes = result[0].boxes.xyxy.cpu().numpy()   # box with xyxy format, (N, 4)\n",
    "\n",
    "    # print(boxes)\n",
    "    for clas, prob, box in zip(cls, probs, boxes):\n",
    "        if prob >= detection_score:\n",
    "            (x, y, x2, y2) = box\n",
    "            x = int(x)\n",
    "            y = int(y)\n",
    "            x2 = int(x2)\n",
    "            y2 = int(y2)\n",
    "\n",
    "            conf = str(round(prob, 2))\n",
    "\n",
    "\n",
    "            # Draw rec\n",
    "            cv2.rectangle(img, (x, y), (x2, y2), class_color_dict[clas], 2)\n",
    "\n",
    "            # Add label text near the bounding box\n",
    "            cv2.putText(img, f'{(class_dict[clas])} : {conf}', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, class_color_dict[clas], 2)\n",
    "\n",
    "\n",
    "    cv2.imwrite(os.path.join('output_pred', OUT_PATH, file_name), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
